{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a note experimenting on the inconsistency issue of LIME and how BayLIME (with 3 options) improves on it -- Image datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:10:44.893395Z",
     "start_time": "2020-08-02T17:10:29.667776Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")# allow the notebook to find the parent folder\n",
    "import keras\n",
    "from keras.applications import inception_v3 as inc_net\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import numpy as np\n",
    "from skimage.segmentation import mark_boundaries\n",
    "#from lime.lime_image import *\n",
    "from lime import lime_image\n",
    "import csv\n",
    "import math\n",
    "from lime import calculate_posteriors\n",
    "print('Notebook run using keras:', keras.__version__)\n",
    "\n",
    "#Here we create a standard InceptionV3 pretrained model \n",
    "#and use it on images by first preprocessing them with the preprocessing tools\n",
    "inet_model = inc_net.InceptionV3()\n",
    "\n",
    "\n",
    "def transform_img_fn(path_list):\n",
    "    out = []\n",
    "    for img_path in path_list:\n",
    "        img = image.load_img(img_path, target_size=(299, 299))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = inc_net.preprocess_input(x)\n",
    "        out.append(x)\n",
    "    return np.vstack(out)\n",
    "\n",
    "\n",
    "images = transform_img_fn([os.path.join('data','5.jpg')])\n",
    "# I'm dividing by 2 and adding 0.5 because of\n",
    "# how this Inception represents images\n",
    "plt.imshow(images[0] / 2 + 0.5)\n",
    "plt.show()\n",
    "preds = inet_model.predict(images)\n",
    "for x in decode_predictions(preds)[0]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:11:12.725717Z",
     "start_time": "2020-08-02T17:10:44.896388Z"
    }
   },
   "outputs": [],
   "source": [
    "explainer = lime_image.LimeImageExplainer(feature_selection='none')#kernel_width=0.1   feature_selection='none'\n",
    "\n",
    "# Hide color is the color for a superpixel turned OFF. Alternatively, if it is NONE, the superpixel will be replaced by the average of its pixels\n",
    "explanation = explainer.explain_instance(images[0], inet_model.predict,\n",
    "                                         top_labels=1, hide_color=0, batch_size=10,\n",
    "                                         num_samples=100,model_regressor='Bay_info_prior')#'non_Bay' 'Bay_non_info_prior' 'Bay_info_prior','BayesianRidge_inf_prior_fit_alpha'\n",
    "\n",
    "# temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=2, hide_rest=True)\n",
    "# plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "# plt.show()\n",
    "\n",
    "# temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=False)\n",
    "# plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "# plt.show()\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "plt.show()\n",
    "print(\"**\")\n",
    "print(explanation.as_list(explanation.top_labels[0]))\n",
    "\n",
    "alpha_init=1\n",
    "lambda_init=1\n",
    "with open('./posterior_configure.csv') as csv_file:\n",
    "    csv_reader=csv.reader(csv_file)\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 1:\n",
    "            alpha_init=float(row[0])\n",
    "            lambda_init=float(row[1])\n",
    "        line_count=line_count+1\n",
    "\n",
    "explanation=calculate_posteriors.get_posterior(explanation,'.\\data\\prior_knowledge_5_jpg.csv' ,hyper_para_alpha=alpha_init, hyper_para_lambda=lambda_init,\n",
    "                                        label=explanation.top_labels[0])\n",
    "\n",
    "\n",
    "\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=5, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "plt.show()\n",
    "\n",
    "print(\"**\")\n",
    "print(explanation.as_list(explanation.top_labels[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:28.946690Z",
     "start_time": "2020-08-02T17:11:12.727752Z"
    }
   },
   "outputs": [],
   "source": [
    "k=3#number of explanations \n",
    "m=63# number of features\n",
    "i=1\n",
    "instance=3\n",
    "explanations=np.array([])\n",
    "while i<=k:\n",
    "    explainer = lime_image.LimeImageExplainer(feature_selection='none')#kernel_width=0.1   feature_selection='none'\n",
    "\n",
    "    exp = explainer.explain_instance(images[0], inet_model.predict,\n",
    "                                         top_labels=1, hide_color=0, batch_size=10,\n",
    "                                         num_samples=100,model_regressor='Bay_info_prior')#'non_Bay' 'Bay_non_info_prior' 'Bay_info_prior','BayesianRidge_inf_prior_fit_alpha'\n",
    "\n",
    "    temp_list=exp.as_list(explanation.top_labels[0])\n",
    "    temp_array = np.array(temp_list)\n",
    "    explanations=np.append(explanations,temp_array)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:28.959655Z",
     "start_time": "2020-08-02T17:12:28.951678Z"
    }
   },
   "outputs": [],
   "source": [
    "exps=explanations.reshape(k,2*m)# k exps, 63 features for this instance.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.004569Z",
     "start_time": "2020-08-02T17:12:28.965157Z"
    }
   },
   "outputs": [],
   "source": [
    "for exp in exps:\n",
    "    #print(exp)\n",
    "    i=1\n",
    "    temp_vector=np.array([])\n",
    "    while i<=(2*m-1):\n",
    "        temp_vector=np.append(temp_vector,float(exp[i]))\n",
    "        i=i+2\n",
    "    #print(temp_vector)\n",
    "    normlised_temp_vector=temp_vector/np.linalg.norm(temp_vector)\n",
    "    #print(normlised_temp_vector)\n",
    "    i=1\n",
    "    while i<=(2*m-1):\n",
    "        exp[i]=normlised_temp_vector[math.floor(i/2)]\n",
    "        i=i+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.031496Z",
     "start_time": "2020-08-02T17:12:29.011551Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_names=np.array([])\n",
    "i=0\n",
    "while i<=(2*m-1):\n",
    "    feature_names=np.append(feature_names,exps[0,i])\n",
    "    i=i+2\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.065407Z",
     "start_time": "2020-08-02T17:12:29.037482Z"
    }
   },
   "outputs": [],
   "source": [
    "def rankings_in_k_exp (feature, k_exps):\n",
    "    ranks=np.array([])\n",
    "    for exp in k_exps:\n",
    "        rank=math.ceil(exp.tolist().index(feature)/2)+1\n",
    "        ranks=np.append(ranks,rank)\n",
    "    return ranks\n",
    "\n",
    "print(rankings_in_k_exp('7',exps))\n",
    "\n",
    "def importance_in_k_exp (feature, k_exps):\n",
    "    importance_s=np.array([])\n",
    "    for exp in k_exps:\n",
    "        importance=exp[exp.tolist().index(feature)+1]\n",
    "        importance_s=np.append(importance_s,importance)\n",
    "    return importance_s\n",
    "\n",
    "print(importance_in_k_exp('7',exps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.158158Z",
     "start_time": "2020-08-02T17:12:29.074383Z"
    }
   },
   "outputs": [],
   "source": [
    "g_i_s=np.array([])\n",
    "f_i_s=np.array([])\n",
    "for feature in feature_names:\n",
    "    g_i=importance_in_k_exp(feature,exps)\n",
    "    f_i=rankings_in_k_exp(feature,exps)\n",
    "    g_i_s=np.append(g_i_s,g_i)\n",
    "    f_i_s=np.append(f_i_s,f_i)\n",
    "g_i_s=g_i_s.reshape(m,k)\n",
    "f_i_s=f_i_s.reshape(m,k)\n",
    "print(g_i_s)\n",
    "print(f_i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.184090Z",
     "start_time": "2020-08-02T17:12:29.164146Z"
    }
   },
   "outputs": [],
   "source": [
    "#now calculate the index of dispersion of ranks for each feature\n",
    "IoD_f_i_s=np.array([])\n",
    "for f_i in f_i_s:\n",
    "    if np.mean(f_i)==0:\n",
    "        IoD_f_i=0\n",
    "        IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "    else:\n",
    "        IoD_f_i=np.var(f_i)/np.mean(f_i)\n",
    "        IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "print(IoD_f_i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.206033Z",
     "start_time": "2020-08-02T17:12:29.189093Z"
    }
   },
   "outputs": [],
   "source": [
    "#now calculate the weighted importance for each feature\n",
    "weights_g_i_s=np.array([])\n",
    "for g_i in g_i_s:\n",
    "    weight=np.mean(abs(g_i.astype(np.float)))\n",
    "    weights_g_i_s=np.append(weights_g_i_s,weight)\n",
    "weights_g_i_s=weights_g_i_s/sum(weights_g_i_s)\n",
    "print(weights_g_i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.223983Z",
     "start_time": "2020-08-02T17:12:29.211019Z"
    }
   },
   "outputs": [],
   "source": [
    "np.dot(weights_g_i_s,IoD_f_i_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.242933Z",
     "start_time": "2020-08-02T17:12:29.228968Z"
    }
   },
   "outputs": [],
   "source": [
    "def kendall_w(expt_ratings):\n",
    "    if expt_ratings.ndim!=2:\n",
    "        raise 'ratings matrix must be 2-dimensional'\n",
    "    m = expt_ratings.shape[0] #raters\n",
    "    n = expt_ratings.shape[1] # items rated\n",
    "    denom = m**2*(n**3-n)\n",
    "    rating_sums = np.sum(expt_ratings, axis=0)\n",
    "    S = n*np.var(rating_sums)\n",
    "    return 12*S/denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:12:29.255898Z",
     "start_time": "2020-08-02T17:12:29.248916Z"
    }
   },
   "outputs": [],
   "source": [
    "ken_w=kendall_w(f_i_s.T)\n",
    "print(ken_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we have shown a toy example of calculating the inconsistency of k explanations for a given number of samples, while we want to see inconsistency as a function of the number of samples... Let us do it now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:10:21.490703Z",
     "start_time": "2020-08-02T17:10:13.184Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "n=10 #number of samples\n",
    "inconsistency_non_info=np.array([])\n",
    "ken_w_non_info=np.array([])\n",
    "while n<=1000:\n",
    "\n",
    "\n",
    "    k=20#number of explanations \n",
    "    m=63# number of features\n",
    "    i=1\n",
    "    explanations=np.array([])\n",
    "    while i<=k:\n",
    "        explainer = lime_image.LimeImageExplainer(feature_selection='none')#kernel_width=0.1   feature_selection='none'\n",
    "\n",
    "        exp = explainer.explain_instance(images[0], inet_model.predict,\n",
    "                                         top_labels=1, hide_color=0, batch_size=10,\n",
    "                                         num_samples=n,model_regressor='Bay_non_info_prior')#'non_Bay' 'Bay_non_info_prior' 'Bay_info_prior','BayesianRidge_inf_prior_fit_alpha'\n",
    "\n",
    "        temp_list=exp.as_list(explanation.top_labels[0])\n",
    "        temp_array = np.array(temp_list)\n",
    "        explanations=np.append(explanations,temp_array)\n",
    "        i=i+1\n",
    "        \n",
    "    exps=explanations.reshape(k,2*m)# k exps, 13 features for this instance.. \n",
    "    for exp in exps:\n",
    "    #print(exp)\n",
    "        i=1\n",
    "        temp_vector=np.array([])\n",
    "        while i<=(2*m-1):\n",
    "            temp_vector=np.append(temp_vector,float(exp[i]))\n",
    "            i=i+2\n",
    "    #print(temp_vector)\n",
    "        normlised_temp_vector=temp_vector/np.linalg.norm(temp_vector)\n",
    "    #print(normlised_temp_vector)\n",
    "        i=1\n",
    "        while i<=(2*m-1):\n",
    "            exp[i]=normlised_temp_vector[math.floor(i/2)]\n",
    "            i=i+2\n",
    "    feature_names=np.array([])\n",
    "    i=0\n",
    "    while i<=(2*m-1):\n",
    "        feature_names=np.append(feature_names,exps[0,i])\n",
    "        i=i+2\n",
    "    \n",
    "    g_i_s=np.array([])\n",
    "    f_i_s=np.array([])\n",
    "    for feature in feature_names:\n",
    "        g_i=importance_in_k_exp(feature,exps)\n",
    "        f_i=rankings_in_k_exp(feature,exps)\n",
    "        g_i_s=np.append(g_i_s,g_i)\n",
    "        f_i_s=np.append(f_i_s,f_i)\n",
    "    g_i_s=g_i_s.reshape(m,k)\n",
    "    f_i_s=f_i_s.reshape(m,k)\n",
    "    IoD_f_i_s=np.array([])\n",
    "    for f_i in f_i_s:\n",
    "        if np.mean(f_i)==0:\n",
    "            IoD_f_i=0\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "        else:\n",
    "            IoD_f_i=np.var(f_i)/np.mean(f_i)\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "    weights_g_i_s=np.array([])\n",
    "    for g_i in g_i_s:\n",
    "        weight=np.mean(abs(g_i.astype(np.float)))\n",
    "        weights_g_i_s=np.append(weights_g_i_s,weight)\n",
    "    weights_g_i_s=weights_g_i_s/sum(weights_g_i_s)\n",
    "    \n",
    "    inconsistency_non_info=np.append(inconsistency_non_info,np.dot(weights_g_i_s,IoD_f_i_s))\n",
    "    \n",
    "    ken_w_non_info=np.append(ken_w_non_info,kendall_w(f_i_s.T))\n",
    "    \n",
    "    print(n)\n",
    "    n=n+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-02T17:10:21.492698Z",
     "start_time": "2020-08-02T17:10:13.869Z"
    }
   },
   "outputs": [],
   "source": [
    "n=10 #number of samples\n",
    "inconsistency_non_Bay=np.array([])\n",
    "ken_w_non_Bay=np.array([])\n",
    "while n<=1000:\n",
    "\n",
    "\n",
    "    k=20#number of explanations \n",
    "    m=63# number of features\n",
    "    i=1\n",
    "    explanations=np.array([])\n",
    "    while i<=k:\n",
    "        explainer = lime_image.LimeImageExplainer(feature_selection='none')#kernel_width=0.1   feature_selection='none'\n",
    "\n",
    "        exp = explainer.explain_instance(images[0], inet_model.predict,\n",
    "                                         top_labels=1, hide_color=0, batch_size=10,\n",
    "                                         num_samples=n,model_regressor='non_Bay')#'non_Bay' 'Bay_non_info_prior' 'Bay_info_prior','BayesianRidge_inf_prior_fit_alpha'\n",
    "\n",
    "        temp_list=exp.as_list(explanation.top_labels[0])\n",
    "        temp_array = np.array(temp_list)\n",
    "        explanations=np.append(explanations,temp_array)\n",
    "        i=i+1\n",
    "        \n",
    "    exps=explanations.reshape(k,2*m)# k exps, 13 features for this instance.. \n",
    "    for exp in exps:\n",
    "    #print(exp)\n",
    "        i=1\n",
    "        temp_vector=np.array([])\n",
    "        while i<=(2*m-1):\n",
    "            temp_vector=np.append(temp_vector,float(exp[i]))\n",
    "            i=i+2\n",
    "    #print(temp_vector)\n",
    "        normlised_temp_vector=temp_vector/np.linalg.norm(temp_vector)\n",
    "    #print(normlised_temp_vector)\n",
    "        i=1\n",
    "        while i<=(2*m-1):\n",
    "            exp[i]=normlised_temp_vector[math.floor(i/2)]\n",
    "            i=i+2\n",
    "    feature_names=np.array([])\n",
    "    i=0\n",
    "    while i<=(2*m-1):\n",
    "        feature_names=np.append(feature_names,exps[0,i])\n",
    "        i=i+2\n",
    "    \n",
    "    g_i_s=np.array([])\n",
    "    f_i_s=np.array([])\n",
    "    for feature in feature_names:\n",
    "        g_i=importance_in_k_exp(feature,exps)\n",
    "        f_i=rankings_in_k_exp(feature,exps)\n",
    "        g_i_s=np.append(g_i_s,g_i)\n",
    "        f_i_s=np.append(f_i_s,f_i)\n",
    "    g_i_s=g_i_s.reshape(m,k)\n",
    "    f_i_s=f_i_s.reshape(m,k)\n",
    "    IoD_f_i_s=np.array([])\n",
    "    for f_i in f_i_s:\n",
    "        if np.mean(f_i)==0:\n",
    "            IoD_f_i=0\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "        else:\n",
    "            IoD_f_i=np.var(f_i)/np.mean(f_i)\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "    weights_g_i_s=np.array([])\n",
    "    for g_i in g_i_s:\n",
    "        weight=np.mean(abs(g_i.astype(np.float)))\n",
    "        weights_g_i_s=np.append(weights_g_i_s,weight)\n",
    "    weights_g_i_s=weights_g_i_s/sum(weights_g_i_s)\n",
    "    \n",
    "    inconsistency_non_Bay=np.append(inconsistency_non_Bay,np.dot(weights_g_i_s,IoD_f_i_s))\n",
    "    \n",
    "    ken_w_non_Bay=np.append(ken_w_non_Bay,kendall_w(f_i_s.T))\n",
    "    \n",
    "    print(n)\n",
    "    n=n+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T15:05:23.483328Z",
     "start_time": "2020-07-31T15:05:23.470326Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"./figures/inconsistency_image_raw_data/lime_incon.csv\", inconsistency_non_Bay, delimiter=\",\")\n",
    "np.savetxt(\"./figures/inconsistency_image_raw_data/lime_ken_w.csv\", ken_w_non_Bay, delimiter=\",\")\n",
    "np.savetxt(\"./figures/inconsistency_image_raw_data/BayLIME_non_info_incon.csv\", inconsistency_non_info, delimiter=\",\")\n",
    "np.savetxt(\"./figures/inconsistency_image_raw_data/BayLIME_non_info_ken_w.csv\", ken_w_non_info, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T02:00:16.766511Z",
     "start_time": "2020-08-02T17:12:29.261882Z"
    }
   },
   "outputs": [],
   "source": [
    "n=10 #number of samples\n",
    "inconsistency_info_prior_fit_alpha=np.array([])\n",
    "ken_w_info_prior_fit_alpha=np.array([])\n",
    "while n<=1000:\n",
    "\n",
    "    k=20#number of explanations \n",
    "    m=63# number of features\n",
    "    i=1\n",
    "    explanations=np.array([])\n",
    "    while i<=k:\n",
    "        explainer = lime_image.LimeImageExplainer(feature_selection='none')#kernel_width=0.1   feature_selection='none'\n",
    "\n",
    "        exp = explainer.explain_instance(images[0], inet_model.predict,\n",
    "                                         top_labels=1, hide_color=0, batch_size=10,\n",
    "                                         num_samples=n,model_regressor='BayesianRidge_inf_prior_fit_alpha')#'non_Bay' 'Bay_non_info_prior' 'Bay_info_prior','BayesianRidge_inf_prior_fit_alpha'\n",
    "        alpha_init=1\n",
    "        lambda_init=1\n",
    "        with open('.\\posterior_configure.csv') as csv_file:\n",
    "            csv_reader=csv.reader(csv_file)\n",
    "            line_count = 0\n",
    "            for row in csv_reader:\n",
    "                if line_count == 1:\n",
    "                    alpha_init=float(row[0])\n",
    "                    lambda_init=float(row[1])\n",
    "                line_count=line_count+1\n",
    "\n",
    "        exp=calculate_posteriors.get_posterior(exp,'.\\data\\prior_knowledge_5_jpg.csv' ,hyper_para_alpha=alpha_init, hyper_para_lambda=lambda_init,\n",
    "                                        label=explanation.top_labels[0])\n",
    "        temp_list=exp.as_list(explanation.top_labels[0])\n",
    "        \n",
    "        temp_array = np.array(temp_list)\n",
    "        explanations=np.append(explanations,temp_array)\n",
    "        i=i+1\n",
    "        \n",
    "    exps=explanations.reshape(k,2*m)# k exps, 63 features for this instance.. \n",
    "    for exp in exps:\n",
    "    #print(exp)\n",
    "        i=1\n",
    "        temp_vector=np.array([])\n",
    "        while i<=(2*m-1):\n",
    "            temp_vector=np.append(temp_vector,float(exp[i]))\n",
    "            i=i+2\n",
    "    #print(temp_vector)\n",
    "        normlised_temp_vector=temp_vector/np.linalg.norm(temp_vector)\n",
    "    #print(normlised_temp_vector)\n",
    "        i=1\n",
    "        while i<=(2*m-1):\n",
    "            exp[i]=normlised_temp_vector[math.floor(i/2)]\n",
    "            i=i+2\n",
    "    feature_names=np.array([])\n",
    "    i=0\n",
    "    while i<=(2*m-1):\n",
    "        feature_names=np.append(feature_names,exps[0,i])\n",
    "        i=i+2\n",
    "    \n",
    "    g_i_s=np.array([])\n",
    "    f_i_s=np.array([])\n",
    "    for feature in feature_names:\n",
    "        g_i=importance_in_k_exp(feature,exps)\n",
    "        f_i=rankings_in_k_exp(feature,exps)\n",
    "        g_i_s=np.append(g_i_s,g_i)\n",
    "        f_i_s=np.append(f_i_s,f_i)\n",
    "    g_i_s=g_i_s.reshape(m,k)\n",
    "    f_i_s=f_i_s.reshape(m,k)\n",
    "    IoD_f_i_s=np.array([])\n",
    "    for f_i in f_i_s:\n",
    "        if np.mean(f_i)==0:\n",
    "            IoD_f_i=0\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "        else:\n",
    "            IoD_f_i=np.var(f_i)/np.mean(f_i)\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "    weights_g_i_s=np.array([])\n",
    "    for g_i in g_i_s:\n",
    "        weight=np.mean(abs(g_i.astype(np.float)))\n",
    "        weights_g_i_s=np.append(weights_g_i_s,weight)\n",
    "    weights_g_i_s=weights_g_i_s/sum(weights_g_i_s)\n",
    "    \n",
    "    inconsistency_info_prior_fit_alpha=np.append(inconsistency_info_prior_fit_alpha,np.dot(weights_g_i_s,IoD_f_i_s))\n",
    "    \n",
    "    ken_w_info_prior_fit_alpha=np.append(ken_w_info_prior_fit_alpha,kendall_w(f_i_s.T))\n",
    "    \n",
    "    n=n+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-03T02:00:16.778678Z",
     "start_time": "2020-08-03T02:00:16.766511Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"./figures/inconsistency_image_raw_data/BayLIME_info_fit_alpha_lambda200_incon.csv\", inconsistency_info_prior_fit_alpha, delimiter=\",\")\n",
    "np.savetxt(\"./figures/inconsistency_image_raw_data/BayLIME_info_fit_alpha_lambda200_ken_w.csv\", ken_w_info_prior_fit_alpha, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-02T17:10:48.420Z"
    }
   },
   "outputs": [],
   "source": [
    "n=10 #number of samples\n",
    "inconsistency_full_info_prior=np.array([])\n",
    "ken_w_full_info_prior=np.array([])\n",
    "while n<=1000:\n",
    "\n",
    "    k=20#number of explanations \n",
    "    m=63# number of features\n",
    "    i=1\n",
    "    explanations=np.array([])\n",
    "    while i<=k:\n",
    "        explainer = lime_image.LimeImageExplainer(feature_selection='none')#kernel_width=0.1   feature_selection='none'\n",
    "\n",
    "        exp = explainer.explain_instance(images[0], inet_model.predict,\n",
    "                                         top_labels=1, hide_color=0, batch_size=10,\n",
    "                                         num_samples=n,model_regressor='Bay_info_prior')#'non_Bay' 'Bay_non_info_prior' 'Bay_info_prior','BayesianRidge_inf_prior_fit_alpha'\n",
    "        alpha_init=1\n",
    "        lambda_init=1\n",
    "        with open('.\\posterior_configure.csv') as csv_file:\n",
    "            csv_reader=csv.reader(csv_file)\n",
    "            line_count = 0\n",
    "            for row in csv_reader:\n",
    "                if line_count == 1:\n",
    "                    alpha_init=float(row[0])\n",
    "                    lambda_init=float(row[1])\n",
    "                line_count=line_count+1\n",
    "\n",
    "        exp=calculate_posteriors.get_posterior(exp,'.\\data\\prior_knowledge_5_jpg.csv' ,hyper_para_alpha=alpha_init, hyper_para_lambda=lambda_init,\n",
    "                                        label=explanation.top_labels[0])\n",
    "        temp_list=exp.as_list(explanation.top_labels[0])\n",
    "        \n",
    "        temp_array = np.array(temp_list)\n",
    "        explanations=np.append(explanations,temp_array)\n",
    "        i=i+1\n",
    "        \n",
    "    exps=explanations.reshape(k,2*m)# k exps, 63 features for this instance.. \n",
    "    for exp in exps:\n",
    "    #print(exp)\n",
    "        i=1\n",
    "        temp_vector=np.array([])\n",
    "        while i<=(2*m-1):\n",
    "            temp_vector=np.append(temp_vector,float(exp[i]))\n",
    "            i=i+2\n",
    "    #print(temp_vector)\n",
    "        normlised_temp_vector=temp_vector/np.linalg.norm(temp_vector)\n",
    "    #print(normlised_temp_vector)\n",
    "        i=1\n",
    "        while i<=(2*m-1):\n",
    "            exp[i]=normlised_temp_vector[math.floor(i/2)]\n",
    "            i=i+2\n",
    "    feature_names=np.array([])\n",
    "    i=0\n",
    "    while i<=(2*m-1):\n",
    "        feature_names=np.append(feature_names,exps[0,i])\n",
    "        i=i+2\n",
    "    \n",
    "    g_i_s=np.array([])\n",
    "    f_i_s=np.array([])\n",
    "    for feature in feature_names:\n",
    "        g_i=importance_in_k_exp(feature,exps)\n",
    "        f_i=rankings_in_k_exp(feature,exps)\n",
    "        g_i_s=np.append(g_i_s,g_i)\n",
    "        f_i_s=np.append(f_i_s,f_i)\n",
    "    g_i_s=g_i_s.reshape(m,k)\n",
    "    f_i_s=f_i_s.reshape(m,k)\n",
    "    IoD_f_i_s=np.array([])\n",
    "    for f_i in f_i_s:\n",
    "        if np.mean(f_i)==0:\n",
    "            IoD_f_i=0\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "        else:\n",
    "            IoD_f_i=np.var(f_i)/np.mean(f_i)\n",
    "            IoD_f_i_s=np.append(IoD_f_i_s,IoD_f_i)\n",
    "    weights_g_i_s=np.array([])\n",
    "    for g_i in g_i_s:\n",
    "        weight=np.mean(abs(g_i.astype(np.float)))\n",
    "        weights_g_i_s=np.append(weights_g_i_s,weight)\n",
    "    weights_g_i_s=weights_g_i_s/sum(weights_g_i_s)\n",
    "    \n",
    "    inconsistency_full_info_prior=np.append(inconsistency_full_info_prior,np.dot(weights_g_i_s,IoD_f_i_s))\n",
    "    \n",
    "    ken_w_full_info_prior=np.append(ken_w_full_info_prior,kendall_w(f_i_s.T))\n",
    "    \n",
    "    n=n+50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-02T17:10:49.100Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"./figures/inconsistency_image_raw_data/BayLIME_full_info_alpha1_lambda200_incon.csv\", inconsistency_full_info_prior, delimiter=\",\")\n",
    "np.savetxt(\"./figures/inconsistency_image_raw_data/BayLIME_full_info_alpha1_lambda200_ken_w.csv\", ken_w_full_info_prior, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T11:22:09.364719Z",
     "start_time": "2020-08-01T11:22:08.879445Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x_index=np.array([])\n",
    "i=10\n",
    "while i<=1000:\n",
    "    x_index=np.append(x_index,i)\n",
    "    i=i+50\n",
    "    \n",
    "\n",
    "plt.plot(x_index,inconsistency_non_Bay,linestyle='-',color='red',label='LIME')\n",
    "plt.plot(x_index,inconsistency_non_info,linestyle='-',color='green',label='BayLIME with non-informative priors')\n",
    "plt.plot(x_index,inconsistency_info_prior_fit_alpha,linestyle='-',color='blue',label=r'BayLIME with partial informative priors ($\\lambda=20$)')\n",
    "plt.plot(x_index,inconsistency_full_info_prior,linestyle='-',color='orange',label=r'BayLIME with full informative priors ($\\alpha=1,\\lambda=20$)')\n",
    "\n",
    "#plt.xscale('log')\n",
    "plt.legend(loc='upper right',fontsize=10)#bbox_to_anchor=(1,1)\n",
    "#plt.axis([10, 300, 0,10])# set the ranges of axis\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('inconsistency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-01T11:22:09.364719Z",
     "start_time": "2020-07-31T16:54:53.788Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_index,ken_w_non_Bay,linestyle='-',color='red',label='LIME')\n",
    "plt.plot(x_index,ken_w_non_info,linestyle='-',color='green',label='BayLIME with non-informative priors')\n",
    "plt.plot(x_index,ken_w_info_prior_fit_alpha,linestyle='-',color='blue',label=r'BayLIME with partial informative priors ($\\lambda=20$)')\n",
    "plt.plot(x_index,ken_w_full_info_prior,linestyle='-',color='orange',label=r'BayLIME with full informative priors ($\\alpha=1,\\lambda=20$)')\n",
    "\n",
    "#plt.xscale('log')\n",
    "plt.legend(fontsize=10)#bbox_to_anchor=(1,1) ,loc='upper right'\n",
    "#plt.axis([10, 300, 0,10])# set the ranges of axis\n",
    "plt.xlabel('$n$')\n",
    "plt.ylabel('Kendall’s W')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
